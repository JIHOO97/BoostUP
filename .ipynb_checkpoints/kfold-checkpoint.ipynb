{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e947fc6a-f3e0-4b8a-af58-827ee8a8d7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.2.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2020.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pandas import DataFrame\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vision_utils\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc12ced-3636-45e5-bceb-7f4fde1d8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = 'model_resnet152_kfold3_epoch2_size0_batch16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18002683-6aa9-426a-9a9f-51c69b700136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeus0007\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">atomic-yogurt-55</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zeus0007/img-classification-38\" target=\"_blank\">https://wandb.ai/zeus0007/img-classification-38</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zeus0007/img-classification-38/runs/382i5yw5\" target=\"_blank\">https://wandb.ai/zeus0007/img-classification-38/runs/382i5yw5</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210830_111411-382i5yw5</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_list=['resnext50_32x4d','resnext101_32x8d','vit_base_patch16_224','my_model','vgg16','resnet152']\n",
    "img_size_x=[512,256]\n",
    "img_size_y=[384,192]\n",
    "wandb.init(project='img-classification-38', entity='zeus0007',config = {\n",
    "    'learning_rate':0.01,\n",
    "    'batch_size':16,\n",
    "    'epoch':2,\n",
    "    'model':'resnet152',\n",
    "    'momentum':0.9,\n",
    "    'img_x':img_size_x[0],\n",
    "    'img_y':img_size_y[0],\n",
    "    'kfold_num':3,\n",
    "})\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e1c93a-bc16-4dc8-9606-1904564d7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "TRAIN_MASK_PATH = {'label':'/opt/ml/input/data/train/train.csv','images':'/opt/ml/input/data/train/images','new':'/opt/ml/input/data/train/new_train.csv'}\n",
    "TEST_MASK_PATH = '/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d01c84-ea0d-49d0-a554-72fe346b08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#심플 트랜스폼\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "def transforms(train=True, img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(img_size[0], img_size[1], p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(img_size[0], img_size[1]),\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fbf10b-cfe6-43d7-a460-9e71eda41c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODOS:데이터셋 만들기\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, df, index, train=True):\n",
    "        # TODOS:csv 가져오기\n",
    "#         data = pd.read_csv(path['new'])\n",
    "        data = df.iloc[index].reset_index(drop=True)\n",
    "        image_path = data['abs_path']\n",
    "        \n",
    "        self.classified_labels = data['class']\n",
    "        self.images_full_path = image_path\n",
    "        \n",
    "        if train :\n",
    "            self.transform =transforms(img_size = (config.img_x,config.img_y),train=True)\n",
    "        else :\n",
    "            self.transform = transforms(img_size = (config.img_x,config.img_y),train=False)\n",
    "        \n",
    "#         self.images = np.array([Image.open(v) for image_full_path in tqdm(images_full_path)])\n",
    "\n",
    "    def set_transform(self,transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images_full_path.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        image_path = self.images_full_path[idx]\n",
    "        image = Image.open('/opt/ml/'+image_path)\n",
    "        y = self.classified_labels[idx]\n",
    "        \n",
    "        X = self.transform(image=np.array(image))['image']\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5e10c9-a3c8-4e9c-825e-d8e437e86e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_MASK_PATH['new'])\n",
    "kfold = KFold(n_splits=config.kfold_num, shuffle=True)\n",
    "\n",
    "def t_dataset():\n",
    "    for train_index, val_index in kfold.split(df):\n",
    "        train_dataset = MaskDataset(df, train_index, train=True)\n",
    "        yield train_dataset\n",
    "def v_dataset():\n",
    "    for train_index, val_index in kfold.split(df):\n",
    "        val_dataset = MaskDataset(df, val_index, train=False)\n",
    "        yield val_dataset\n",
    "train_datasets = t_dataset()\n",
    "val_datasets = v_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9d1e34-2a7d-4980-a8e5-bedaff4b99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_model(model):\n",
    "    if model == 'resnext50_32x4d':\n",
    "        return models.resnext50_32x4d(pretrained=True).to(device)\n",
    "    elif model == 'resnext101_32x8d':\n",
    "        return models.resnext101_32x8d(pretrained=True).to(device)\n",
    "    elif model == 'vit_base_patch16_224':\n",
    "        return timm.create_model('vit_base_patch16_224',pretrained=True).to(device)\n",
    "    elif model == 'vgg16':\n",
    "        return models.vgg16(pretrained=True).to(device)\n",
    "    elif model == 'resnet152':\n",
    "        return models.resnet152(pretrained=True).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d841459f-9c20-4548-89b8-fd6db4b90106",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_model(config.model)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "418cf85b-111a-443e-9335-c76e22152365",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config.learning_rate\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=config.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81710f19-4dee-44e9-9779-a9fb2492bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=1):\n",
    "    for i, (train_dataset, val_dataset) in enumerate(zip(train_datasets,val_datasets)):\n",
    "        print(f'fold : {i+1}')\n",
    "        print('-' * 10)\n",
    "        image_datasets = {'train':train_dataset,'validation':val_dataset}\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=4,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=4,\n",
    "            shuffle=False\n",
    "        )\n",
    "        dataloaders = {'train':train_loader, 'validation':val_loader}\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'validation']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_loss / len(image_datasets[phase])\n",
    "                epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "                wandb.log({f\"{phase}_acc\":epoch_acc, f\"{phase}_loss\":epoch_loss})\n",
    "                print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                            epoch_loss,\n",
    "                                                            epoch_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15839414-70fc-45ae-b6ec-07b02df1110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b9581-4f82-48a8-89c3-77d681712b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9212d20546c24909b8516a94ca7d62a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=788.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.7305, acc: 0.7522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1e483354fb45b395bbf261528d61b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=394.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss: 0.4354, acc: 0.8352\n",
      "Epoch 2/2\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c9ad592f6643ea803388f782d82022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=788.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import transforms as simple_transforms\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = simple_transforms.Compose([\n",
    "    Resize((config, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "model = train_model(model, criterion, optimizer, num_epochs=config.epoch)\n",
    "model.eval()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a3e0d-eb4b-435c-bde9-a1f4306c1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'/opt/ml/code/model/{TITLE}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31bfb7-d5b0-4954-bdb0-a7ff8310a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=(0.548, 0.504, 0.479)\n",
    "std=(0.237, 0.247, 0.246)\n",
    "def imshow(input, title):\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ee37d-a5d9-430b-822a-c6e8a7551c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vision_utils\n",
    "\n",
    "# Test gender model\n",
    "model.eval()\n",
    "class_names = [0, 1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, inputs in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        images = vision_utils.make_grid(inputs)\n",
    "        imshow(images.cpu(), title=preds)\n",
    "            \n",
    "        if i == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d6577-7a38-4640-9d94-6f47736b2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
