{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dc383e-20e7-4a7c-afe3-2c30c5e48c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # import sys\n",
    "# # from glob import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# # import cv2\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from PIL import Image\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm.notebook import tqdm\n",
    "# # from time import time\n",
    "# from matplotlib import image\n",
    "# from torchvision import transforms, datasets, models\n",
    "# from torchvision import models\n",
    "# import time\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74eb4bf-b7e4-4285-b7d4-0b32cd4fcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pandas import DataFrame\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vision_utils\n",
    "import timm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0945a830-f7b7-426f-9ec2-65bc72f5e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/new_train.csv'\n",
    "gdf_path = f'{data_dir}/gray_train.csv'\n",
    "model_path = f'{data_dir}/models'\n",
    "gs_img_dir = f'{data_dir}/gray_scale_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a9bff1-f575-4194-b6f1-da5997284f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/opt/ml/input/data/train/new_train.csv')\n",
    "# df\n",
    "# '/opt/ml/input/data/train/new_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb8af4-de38-4314-b029-8098855ee7ff",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c139c28-db69-40ad-a631-24e0bcebf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "\n",
    "    def __init__(self,path):\n",
    "        df = pd.read_csv(path)\n",
    "        self.X = df['abs_path']\n",
    "        self.y = df['class']\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        label = self.y[index]\n",
    "        image = Image.open('/opt/ml/'+self.X[index])\n",
    "        \n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "#         image_transform = self.transform(image)\n",
    "        return image_transform, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504464f-b015-4960-8889-89637db32a89",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b69e2e3-b73d-4283-98ad-cb751a2e7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from albumentations import *\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "#     \"\"\"\n",
    "#     train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "#     Args:\n",
    "#         need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "#         img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "#         mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "#         std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "#     Returns:\n",
    "#         transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "#     \"\"\"\n",
    "#     transformations = {}\n",
    "#     if 'train' in need:\n",
    "#         transformations['train'] = Compose([\n",
    "#             Resize(img_size[0], img_size[1], p=1.0),\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             ShiftScaleRotate(p=0.5),\n",
    "#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "#             RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "#             GaussNoise(p=0.5),\n",
    "#             Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "#             ToTensorV2(p=1.0),\n",
    "#         ], p=1.0)\n",
    "#     if 'val' in need:\n",
    "#         transformations['val'] = Compose([\n",
    "#             Resize(img_size[0], img_size[1]),\n",
    "#             Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "#             ToTensorV2(p=1.0),\n",
    "#         ], p=1.0)\n",
    "#     return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef3277b-933e-44db-9f92-126043872928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(216, 192), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = A.Compose([\n",
    "            A.Resize(img_size[0], img_size[1], p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = A.Compose([\n",
    "            A.Resize(img_size[0], img_size[1]),\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439770f1-3e02-4dfe-9bd4-500121151f09",
   "metadata": {},
   "source": [
    "### 2nd transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b29cfd-604a-4fc1-a099-e63c3546450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.56019068, 0.52409788, 0.50145447]\n",
    "std = [0.23318342, 0.24299835, 0.24567397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8381bea-9706-418f-820e-907edaa48828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# transforms_train = transforms.Compose([\n",
    "# #     transforms.Resize((512, 384)),\n",
    "#     transforms.Resize((256,192)),\n",
    "#     transforms.RandomHorizontalFlip(), # data augmentation\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std), # normalization\n",
    "# ])\n",
    "\n",
    "# transforms_val = transforms.Compose([\n",
    "#     transforms.Resize((256,192)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835db30e-371b-48fe-b364-12cc0d1d0bb3",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d05fbd-af3d-4d25-b125-337c4f2967a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset = BaseDataset('/opt/ml/input/data/train/new_train.csv')\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset.dataset.set_transform(transform['train'])\n",
    "val_dataset.dataset.set_transform(transform['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eeb042-c49a-4fdf-9938-0a6f75062e75",
   "metadata": {},
   "source": [
    "### second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48e51cee-5101-4793-9cdf-674d3c8eacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = BaseDataset()\n",
    "\n",
    "# # train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "# n_val = int(len(dataset) * 0.2)\n",
    "# n_train = len(dataset) - n_val\n",
    "# train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# # 각 dataset에 augmentation 함수를 설정합니다.\n",
    "# train_dataset.dataset.set_transform(transforms_train)\n",
    "# val_dataset.dataset.set_transform(transforms_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007bd08-5c82-4a83-aa28-d1e2f701a9f2",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c67dc35-aeba-45a9-ab9b-e023fdbd383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af074ce6-3dc0-4065-a9c6-0a9224f4bcd2",
   "metadata": {},
   "source": [
    "# Convert to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25686c38-2dc1-4661-a133-d09d5f4877a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d01c2b-8963-41b7-b002-37bf98fca4ce",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f098751c-e0b1-45d6-a3ee-5da24f918236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet152(pretrained=True).to(device)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 18).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6048300-7519-4ee9-84b6-68f3314a276a",
   "metadata": {},
   "source": [
    "### 강의 transformer (epoch = 8) and simple transformer (epohc = 7)\n",
    "### resnet152 (epoch = 7, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2af0609d-fdc8-4033-88b7-39a3c241f880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46151c6326d4ab8b8e4368c79def32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=237.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a901a100657e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 7\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\" Training Phase \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    # load a batch data of images\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # forward inputs and get output\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "\n",
    "        # get loss value and update the network weights\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects / len(train_dataset)\n",
    "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}%'.format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "    \"\"\" Validation Phase \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects / len(val_dataset) * 100.\n",
    "        print('[Validation #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e946fd-cdef-40b9-a331-caa1987976a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), os.path.join(model_path, 'model_resnet152_transform.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3ff6a-3a19-47cc-8fc2-e47a663e5b12",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b40888-dbf9-4945-902f-b0b127af53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebf9c7-3a07-467a-8821-fa6373131214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1d60d-11d2-4483-91a4-b09f70aa49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5018d37-5644-49df-b297-d5f0ea8e9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89f458-77a9-4ab5-bd66-3a9597a2d2ed",
   "metadata": {},
   "source": [
    "### Test with eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ab39e-240a-47b8-bca9-453009546e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(input, title):\n",
    "    # torch.Tensor => numpy\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    # undo image normalization\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # display images\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1610b-c573-4880-aeea-506001623aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test gender model\n",
    "import torchivision\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "class_names = [0, 1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, inputs in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        images = torchvision.utils.make_grid(inputs)\n",
    "        imshow(images.cpu(), title=preds)\n",
    "            \n",
    "        if i == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0f63b-97a7-4744-87be-8e40ddc1710b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined model\n",
    "model.eval()\n",
    "all_predictions= []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        all_predictions.append(predicted)\n",
    "        print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1639cd6-57d6-4f44-af83-fcfef46857fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_predictions)):\n",
    "    all_predictions[i] = int(all_predictions[i])\n",
    "\n",
    "print(all_predictions[:5])\n",
    "print(len(all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64905f-bb75-4a5b-ab03-fc1ba39528d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission1.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
