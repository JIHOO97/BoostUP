{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ad50fa-1d8b-47aa-a6f5-a699818ff80a",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "1c9cd972-9001-4fec-9550-daf1162dc306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15171<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from matplotlib import image\n",
    "from torchvision import transforms, datasets, models\n",
    "from vit_pytorch import ViT\n",
    "from vit_pytorch.distill import DistillableViT, DistillWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbda29f-71b3-462e-9210-1ffdde324f18",
   "metadata": {},
   "source": [
    "# 완득이 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "7ed31330-4491-4753-b6c9-8eb29ad3eba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3vcjrxgx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15140<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/wandb/run-20210830_150225-3vcjrxgx/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/wandb/run-20210830_150225-3vcjrxgx/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">peach-plant-15</strong>: <a href=\"https://wandb.ai/jihoo/img-classification-38/runs/3vcjrxgx\" target=\"_blank\">https://wandb.ai/jihoo/img-classification-38/runs/3vcjrxgx</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3vcjrxgx). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sunny-thunder-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jihoo/img-classification-38\" target=\"_blank\">https://wandb.ai/jihoo/img-classification-38</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jihoo/img-classification-38/runs/24eebqbh\" target=\"_blank\">https://wandb.ai/jihoo/img-classification-38/runs/24eebqbh</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/wandb/run-20210830_150427-24eebqbh</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='img-classification-38', \n",
    "           entity='jihoo',\n",
    "           config = \n",
    "           {\n",
    "               'img_size': [512, 384],\n",
    "               'mean': [0.56019068, 0.52409788, 0.50145447],\n",
    "               'std': [0.23318342, 0.24299835, 0.24567397],\n",
    "               'learning_rate':0.001,\n",
    "               'momentum': 0.9,\n",
    "               'batch_size': 32,\n",
    "               'num_workers': 4,\n",
    "               'epoch': 7,\n",
    "               'model':'resnet34'\n",
    "           })\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89f46046-0617-4350-ba2b-da28a51be020",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3lfjg0vy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13851<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/wandb/run-20210830_084728-3lfjg0vy/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/wandb/run-20210830_084728-3lfjg0vy/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_acc</td><td>99.79498</td></tr><tr><td>train_loss</td><td>0.00969</td></tr><tr><td>_runtime</td><td>1046</td></tr><tr><td>_timestamp</td><td>1630314294</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>val_acc</td><td>98.86243</td></tr><tr><td>val_loss</td><td>0.04264</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_acc</td><td>▁▆▇███</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁</td></tr><tr><td>_runtime</td><td>▁▃▃▄▄▅▅▇▇██</td></tr><tr><td>_timestamp</td><td>▁▃▃▄▄▅▅▇▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>val_acc</td><td>▁▄▇▂█</td></tr><tr><td>val_loss</td><td>█▅▂█▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">leafy-sea-9</strong>: <a href=\"https://wandb.ai/jihoo/img-classification-38/runs/3lfjg0vy\" target=\"_blank\">https://wandb.ai/jihoo/img-classification-38/runs/3lfjg0vy</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3lfjg0vy). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">glorious-sound-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jihoo/img-classification-38\" target=\"_blank\">https://wandb.ai/jihoo/img-classification-38</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jihoo/img-classification-38/runs/dj1wda58\" target=\"_blank\">https://wandb.ai/jihoo/img-classification-38/runs/dj1wda58</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/wandb/run-20210830_091837-dj1wda58</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb.init(project='img-classification-38', \n",
    "#            entity='jihoo',\n",
    "#            config = \n",
    "#            {\n",
    "#                'img_size': [256, 192],\n",
    "#                'mean': [0.5, 0.5, 0.5],\n",
    "#                'std': [0.2, 0.2, 0.2],\n",
    "#                'learning_rate':0.01,\n",
    "#                'momentum': 0.5,\n",
    "#                'batch_size': 32,\n",
    "#                'num_workers': 4,\n",
    "#                'epoch': 5,\n",
    "#                'model':'resnet152'\n",
    "#            })\n",
    "# config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479bb11-1a92-4056-9858-32d8227f78e6",
   "metadata": {},
   "source": [
    "# Path Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "edbb4982-7f06-4d15-bd89-822c0ebbd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = 'input/data/train'\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/csvs/new_train.csv'\n",
    "model_path = f'{data_dir}/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c10cbf-6d94-4206-975b-ffb852bec770",
   "metadata": {},
   "source": [
    "# Import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c2bc5edc-e72b-415f-8e8b-41d611cabb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Path</th>\n",
       "      <th>NewClass</th>\n",
       "      <th>-</th>\n",
       "      <th>Class Compare</th>\n",
       "      <th>Original Class</th>\n",
       "      <th>-.1</th>\n",
       "      <th>Mask</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>-.2</th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Gender.1</th>\n",
       "      <th>Age.1</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>incorrect_mask.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask1.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask2.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask3.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask4.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Full Path  NewClass   -  \\\n",
       "0  /opt/ml/input/data/train/images/000001_female_...        10 NaN   \n",
       "1  /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "2  /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "3  /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "4  /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "\n",
       "   Class Compare  Original Class  -.1  Mask  Gender  Age  -.2  \\\n",
       "0           True              10  NaN     1       1    1  NaN   \n",
       "1           True               4  NaN     0       1    1  NaN   \n",
       "2           True               4  NaN     0       1    1  NaN   \n",
       "3           True               4  NaN     0       1    1  NaN   \n",
       "4           True               4  NaN     0       1    1  NaN   \n",
       "\n",
       "            ImageName Gender.1  Age.1 ID  \n",
       "0  incorrect_mask.jpg   female     45  1  \n",
       "1           mask1.jpg   female     45  1  \n",
       "2           mask2.jpg   female     45  1  \n",
       "3           mask3.jpg   female     45  1  \n",
       "4           mask4.jpg   female     45  1  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffca5f-76d3-4be8-ab9d-e1c00c1b5346",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "73b04d4c-2bd3-48e6-81f5-882e8289afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        \n",
    "        self.X = df['Full Path']\n",
    "        self.y = df['NewClass']\n",
    "        self.transform = transform\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = Image.open(self.X[index])\n",
    "        label = self.y[index]\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        # image_transform = self.transform(image=np.array(image))['image']\n",
    "        image_transform = self.transform(image)\n",
    "        return image_transform, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b634a-0195-45aa-9daf-25bba602f84c",
   "metadata": {},
   "source": [
    "### Apply transform for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "85402d5b-ac54-438d-acbb-d6bdca0b69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        \n",
    "        self.X = df['Full Path']\n",
    "        self.y = df['NewClass']\n",
    "        self.transform = transform\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = Image.open(self.X[index])\n",
    "        label = self.y[index]\n",
    "        \n",
    "        tmp_image = np.array(image)\n",
    "        image_mean = tmp_image.mean(axis=(0,1)) / 255.\n",
    "        image_std = tmp_image.std(axis=(0,1)) / 255.\n",
    "        \n",
    "        image_transform = self.transform(mean=image_mean, std=image_std)(image)\n",
    "            \n",
    "        return image_transform, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b723e-dde4-40f1-bd28-2c05b8edc604",
   "metadata": {},
   "source": [
    "# Apply transformation to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c61ff-b7a8-4cab-b560-c6de9f0616e1",
   "metadata": {},
   "source": [
    "### functions to apply transform to each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d6ed7-4e20-4f79-b8f1-6b9a2509d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_transforms(mean=(0.5,0.5,0.5), std=(0.2,0.2,0.2)):\n",
    "#     transforms_train = transforms.Compose([\n",
    "#         transforms.Resize((img_size[0], img_size[1])),\n",
    "#         transforms.RandomHorizontalFlip(), # data augmentation\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std) # normalization\n",
    "#     ])\n",
    "#     return transforms_train\n",
    "\n",
    "# def get_val_transforms(mean=(0.5,0.5,0.5), std=(0.2,0.2,0.2)):\n",
    "#     transforms_val = transforms.Compose([\n",
    "#         transforms.Resize((img_size[0], img_size[1])),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)\n",
    "#     ])\n",
    "#     return transforms_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6a36f-db54-4434-9e60-86ee43756c64",
   "metadata": {},
   "source": [
    "### One single tranform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "998f7d32-3316-4d81-b1b3-fd96276f4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransformedDataset(img_size, mean, std):\n",
    "    transforms_train = transforms.Compose([\n",
    "        transforms.Resize((img_size[0], img_size[1])),\n",
    "        transforms.RandomHorizontalFlip(), # data augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std) # normalization\n",
    "    ])\n",
    "\n",
    "    transforms_val = transforms.Compose([\n",
    "        transforms.Resize((img_size[0], img_size[1])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    train_dataset.dataset.set_transform(transforms_train)\n",
    "    val_dataset.dataset.set_transform(transforms_val)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a53361-0212-4b44-ae6f-78e178ca7f6b",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f94829b3-621f-41a8-9f5c-07462aca7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BaseDataset()\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88a69f-6d4c-4525-918e-086ae4f63617",
   "metadata": {},
   "source": [
    "### Apply mean and std for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397c47b-793b-498b-8d83-562191f725fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.dataset.set_transform(get_train_transforms)\n",
    "# val_dataset.dataset.set_transform(get_val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073ec9d-072a-4651-a12a-e6bb6edb5bff",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9eb39091-6e8d-4221-90b4-9154cfe386fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoader(batch_size, num_workers):\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14e6da-88f0-4fe5-844d-2f63c551b31a",
   "metadata": {},
   "source": [
    "# Convert to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9027a7e6-17a7-4d2a-87a2-147aec05c532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f667ec3-2168-4dbf-b66e-177ab2aea545",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "370c4b2f-d14b-4eb4-b067-a24b4198f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name, lr, momentum):\n",
    "    if model_name == 'resnet152':\n",
    "        model = models.resnet152(pretrained=True)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = models.resnet34(pretrained=True)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 18)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd7fe8-5727-43c1-9981-4f9ea432669a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "910e3151-8d60-4c29-b6e3-9ccf6ea84483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, criterion, optimizer):\n",
    "    num_epochs = epoch\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\" Training Phase \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "\n",
    "        # load a batch data of images\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward inputs and get output\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # get loss value and update the network weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects / len(train_dataset) * 100.\n",
    "        wandb.log({\"train_acc\":epoch_acc, \"train_loss\":epoch_loss})\n",
    "        print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time() - start_time))\n",
    "\n",
    "        \"\"\" Validation Phase \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(val_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(val_dataset)\n",
    "            epoch_acc = running_corrects / len(val_dataset) * 100.\n",
    "            wandb.log({\"val_acc\":epoch_acc, \"val_loss\":epoch_loss})\n",
    "            print('[Validation #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9bdcf-1bed-4076-b908-c5a11d73313e",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "79f8f282-7a19-4f82-8152-b3b4e2163912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, model_name):\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f'{model_name}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205bcb9-8a8f-4bca-a110-eb2113797218",
   "metadata": {},
   "source": [
    "# Train the model with different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4006932-305d-43a7-9087-9586d1c4579f",
   "metadata": {},
   "source": [
    "### Get the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ac7e6de7-b276-427a-b38f-12cf426a323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_size': [512, 384], 'mean': [0.56019068, 0.52409788, 0.50145447], 'std': [0.23318342, 0.24299835, 0.24567397], 'learning_rate': 0.001, 'momentum': 0.9, 'batch_size': 32, 'num_workers': 4, 'epoch': 7, 'model': 'resnet34'}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = config.img_size\n",
    "mean = config.mean\n",
    "std = config.std\n",
    "lr = config.learning_rate\n",
    "momentum = config.momentum\n",
    "batch_size = config.batch_size\n",
    "num_workers = config.num_workers\n",
    "epoch = config.epoch\n",
    "model_name = config.model\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e704144-c931-4b3d-b03a-5290d5deb9d8",
   "metadata": {},
   "source": [
    "### Get dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ec3b97a5-5602-4b7e-a31e-07e07f71bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One single mean and std\n",
    "train_dataset, val_dataset = getTransformedDataset(img_size, mean, std)\n",
    "train_loader, val_loader = getDataLoader(batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263bdaca-fe9c-4923-b78a-5d855f533473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate mean and std\n",
    "# train_loader, val_loader = getDataLoader(batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11ec36-58d2-45d2-b8e5-274e5a4d9cba",
   "metadata": {},
   "source": [
    "### Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "71b8df99-3b42-4d65-b724-c54a93b47fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer = getModel(model_name, lr, momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e4eed-da08-47a6-b740-6d8adbf15f9c",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3a273068-5593-457a-935f-37eded413eeb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45679fd1edc44e9aade83914a52c79b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #0] Loss: 0.8742 Acc: 74.5106% Time: 112.0695s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2312babb25a4483c93955381b7d91027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #0] Loss: 0.3927 Acc: 87.0900% Time: 125.4185s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806d37faaee64e0fa31aeb82cf1fdcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #1] Loss: 0.2690 Acc: 91.0979% Time: 237.6095s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7124f68a21423ab57597b027c7b83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #1] Loss: 0.2088 Acc: 92.8042% Time: 251.0033s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc76d08800d4db88210115b98e39332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #2] Loss: 0.1395 Acc: 95.8201% Time: 362.9585s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eb70b02e644c4483a5e1efb924fcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #2] Loss: 0.1349 Acc: 95.5556% Time: 376.2866s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56ac56214e74f2bb8212cd92d6bbe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #3] Loss: 0.0745 Acc: 98.0886% Time: 487.8707s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1e1722470d45c0ac8e42438a223542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #3] Loss: 0.1129 Acc: 95.9259% Time: 501.0990s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa2249dd2d34555aebb907fb5065f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #4] Loss: 0.0418 Acc: 99.2130% Time: 612.9065s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23862b3eeb2a461ebca3f9fb90435589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #4] Loss: 0.0581 Acc: 98.1217% Time: 626.2615s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd44c2ef79f41d5bf37add2c4c028fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #5] Loss: 0.0241 Acc: 99.6561% Time: 737.6450s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896a634105ce49609d3379f84bf8c90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #5] Loss: 0.0616 Acc: 97.9894% Time: 750.9614s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95718ea3fd54f9ea8acd152b38f745a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=473.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #6] Loss: 0.0149 Acc: 99.9074% Time: 862.7052s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8131f9fb194b8e9d050d47fdc78d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #6] Loss: 0.0484 Acc: 98.4392% Time: 875.9744s\n"
     ]
    }
   ],
   "source": [
    "train(model, epoch, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26167f8d-58b2-4172-93d1-28fd7154678e",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "b531f326-ce5d-4e7c-b789-05cc852ed44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saveModel(model, 'resnet34_epoch7_simpletransform(nd)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d48a6-5e1e-465a-a153-d12037081e1b",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65f1db-5461-430d-b092-7a5c284c9206",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "7df0fde0-0690-48be-b50a-6125a7c73c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f41a54-efed-47c6-b028-6352f67fa0fa",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2dd7979f-ff70-4c3e-abf1-0ffdc1e7abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "#             tmp_image = np.array(image)\n",
    "#             image_mean = tmp_image.mean(axis=(0,1)) / 255.\n",
    "#             image_std = tmp_image.std(axis=(0,1)) / 255.\n",
    "#             image = self.transform(mean=image_mean, std=image_std)(image)\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43b957-90f0-4dba-8db6-53f857039b12",
   "metadata": {},
   "source": [
    "### Submission csv file path and eval image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "cd65db44-0cd0-4d82-b7cb-f02d6cb2debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd1b12-e983-49f3-a69e-a9c594117d6e",
   "metadata": {},
   "source": [
    "### Get Test dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "1bf74d6d-c5d7-4869-8066-8200f7f41af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((img_size[0], img_size[1]), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b238c-beb1-4aaf-ba1e-2d900c8e1c2c",
   "metadata": {},
   "source": [
    "### Transform function for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "11a91df3-f3ab-4b57-951c-cbd45cc36e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_transforms(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)):\n",
    "#     transform = transforms.Compose([\n",
    "#         Resize((img_size[0], img_size[1]), Image.BILINEAR),\n",
    "#         ToTensor(),\n",
    "#         Normalize(mean=mean, std=std),\n",
    "#     ])\n",
    "#     return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a0996917-106e-4947-ad0b-e9d2eb1e01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TestDataset(image_paths, get_transforms)\n",
    "\n",
    "# loader = DataLoader(\n",
    "#     dataset,\n",
    "#     shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016691f5-96a2-4b09-a19a-adf0b139f2c4",
   "metadata": {},
   "source": [
    "### Show predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "e5f13868-da0b-4ef8-8ed1-4ab8758a0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(input, title):\n",
    "    # torch.Tensor => numpy\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    # undo image normalization\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # display images\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ccd35c-f050-4f4f-a573-f01acfa5547b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a7eda157b57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test gender model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Test gender model\n",
    "model.eval()\n",
    "start_time = time()\n",
    "class_names = [0, 1]\n",
    "\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    cnt += 1\n",
    "    \n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        images = torchvision.utils.make_grid(inputs)\n",
    "        imshow(images.cpu(), title=preds)\n",
    "            \n",
    "        if cnt == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198ac4b-691b-42b0-bfab-449ee18c63a4",
   "metadata": {},
   "source": [
    "### Save predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49bbc4a-cdcd-460f-be1b-4af0ceb60b9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc61b2b3e48c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# combined model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# combined model\n",
    "model.eval()\n",
    "all_predictions= []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = int(predicted.cpu().numpy())\n",
    "        all_predictions.append(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4b20b-e939-44cf-9454-e4c23187972d",
   "metadata": {},
   "source": [
    "### Save submission csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "a16d295d-889a-4c85-bb11-cabb036fae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
