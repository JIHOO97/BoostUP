{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPhRpnGbllOL"
   },
   "source": [
    "# Assignment 1: Gradient Descent\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "본 과제에서는 경사하강법(Gradient Descent)를 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rPsh3Gm07LV"
   },
   "source": [
    "## 1. Gradient Descent (1)\n",
    "\n",
    "먼저 [SymPy library](https://www.sympy.org/en/index.html)를 사용하여 간단한 이차함수의 최솟값을 gradient descent 방법으로 찾는 문제입니다. - AI Math 3강 참고\n",
    "\n",
    "(SymPy: 수학 방정식의 기호(symbol)를 사용하게 해 주는 라이브러리)\n",
    "\n",
    " #### **def func**\n",
    "- `sym.poly` 함수는 함수식을 정의해줍니다.\n",
    "\n",
    "- `sym.subs` 함수는 변수를 다른변수로 치환하거나 값을 대입해줍니다.\n",
    "\n",
    "\n",
    "#### **func_gradient**\n",
    "- `sym.diff` 함수는 도함수를 구해줍니다.\n",
    "- `func` 함수를 사용하여 미분값과 함수를 return하는 코드를 짜야합니다.\n",
    "\n",
    "#### **gradient_descent**\n",
    "- `init_point`는 경사하강법의 시작점을 의미합니다.\n",
    "- `lr_rate`는 learning rate로 step의 크기를 정해줍니다.\n",
    "- `epsilon`은 gradient 크기의 lower bound입니다.\n",
    "- init_point부터 경사하강법을 시작해서, 최소점이 출력될 수 있도록 코드를 짜야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4bnWEgmKuJ1J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sym\n",
    "from sympy.abc import x\n",
    "from sympy.plotting import plot\n",
    "\n",
    "def func(val):\n",
    "    fun = sym.poly(x**2 + 2*x + 3)\n",
    "    return fun.subs(x, val), fun\n",
    "\n",
    "def func_gradient(fun, val):\n",
    "    diff = sym.diff(fun(val)[1])\n",
    "    return diff.subs(x, val), diff\n",
    "\n",
    "def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):\n",
    "    cnt = 0\n",
    "    val = init_point\n",
    "    grad = func_gradient(fun,val)[0]\n",
    "    while(abs(grad)>epsilon):\n",
    "        val = val - lr_rate * grad\n",
    "        grad = func_gradient(fun,val)[0]\n",
    "    \n",
    "    print(\"함수: {}\\n연산횟수: {}\\n최소점: ({}, {})\".format(fun(val)[1], cnt, val, fun(val)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bROZY7uHCx3X"
   },
   "source": [
    "- 최종적으로 함수, 연산횟수, 최소점(-1,2)이 출력되면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YFdjdR5vC0bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "함수: Poly(x**2 + 2*x + 3, x, domain='ZZ')\n",
      "연산횟수: 0\n",
      "최소점: (-0.999995020234038, 2.00000000002480)\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(fun = func, init_point = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udMVpeMaXJhB"
   },
   "source": [
    "## 2. Gradient Descent (2)\n",
    "\n",
    "- 위의 예제에서 (-1,2)와 거의 동일한 최소점을 얻으셨나요? 그럼 이번에는 sympy library를 사용하지 않고 직접 Gradient Descent를 구현해봅시다!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1ygwhh4m2ex"
   },
   "source": [
    "$$ f'(x) = \\lim_{h \\rightarrow\n",
    " 0} \\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nYj3r-Pm1Ln"
   },
   "source": [
    "- 원래의 미분 공식은 위와 같이 h를 0의 극한으로 보내면 되지만, 컴퓨터 상에서는 불가능하기 때문에 h에 1e-9와 같이 아주 작은 수를 넣어줌으로써 유사한 변화율을 구합니다.\n",
    "\n",
    "#### **difference_quotient(f, x, h)**\n",
    "- h만큼 움직였을 때의 변화율을 계산해주는 코드입니다.\n",
    "- h는 1e-9와 같이 매우 작은 수가 들어갑니다.\n",
    "- f에는 아래에 정의된 func 함수가 들어가고, x에는 변화율을 계산할 point가 들어갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "x4Lj4vl1KEDn"
   },
   "outputs": [],
   "source": [
    "def func(val):\n",
    "    fun = sym.poly(x**2 + 2*x + 3)\n",
    "    return fun.subs(x, val)\n",
    "\n",
    "def difference_quotient(f, x, h=1e-9):\n",
    "    result = (f(x+h)-f(x))/h\n",
    "    return result\n",
    "\n",
    "def gradient_descent(func, init_point, lr_rate=1e-2, epsilon=1e-5):\n",
    "    cnt = 0\n",
    "    val = init_point\n",
    "    grad = difference_quotient(func,val)\n",
    "    while abs(grad) > epsilon:\n",
    "        val = val - lr_rate*grad\n",
    "        grad = difference_quotient(func,val)\n",
    "\n",
    "    print(\"연산횟수: {}\\n최소점: ({}, {})\".format(cnt, val, func(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HzMqhLxC60r"
   },
   "source": [
    "- 최종적으로 연산횟수, 최소점(-1,2)이 출력되면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rg_K54iaC-rS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연산횟수: 0\n",
      "최소점: (-0.999994846459742, 2.00000000002656)\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(func, init_point=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7C4-EzC4azr"
   },
   "source": [
    "## 3. Linear Regression\n",
    "- sympy library를 사용했을 때와 비슷한 결과값을 얻었나요?\n",
    "- 그럼 이제 본격적으로 Gradient Descent를 활용한 Linear Regression을 구현해 봅시다.\n",
    "\n",
    "### **3-1. Basic function**\n",
    "#### **Dataset 1** : train_x, train_y\n",
    "$$y = wx + b$$ $$y = 7x + 2$$\n",
    "- 위의 식을 알아내기 위해서 train_x, train_y의 data point를 가지고 Linear Regression을 진행합니다.\n",
    "\n",
    "#### **Gradient Descent**\n",
    "- error를 어떻게 계산하고, gradient를 통해 w와 b를 어떻게 조정해 나가는지에 대한 코드를 짜주시면 됩니다.\n",
    "    - error는 L2 norm으로 사용하고, np.sum 함수를 활용하시면 됩니다.\n",
    "- 결과값이 (7,2)와 유사하게 출력이 되면 성공입니다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Y4yOk4jR4aog"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sympy' has no attribute 'subs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-56d228312136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgradient_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mgradient_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'subs'"
     ]
    }
   ],
   "source": [
    "train_x = (np.random.rand(1000) - 0.5) * 10\n",
    "train_y = np.zeros_like(train_x)\n",
    "\n",
    "def func(val):\n",
    "    fun = sym.poly(7*x + 2)\n",
    "    return fun.subs(x, val)\n",
    "\n",
    "for i in range(1000):\n",
    "    train_y[i] = func(train_x[i])\n",
    "\n",
    "# initialize\n",
    "w, b = 0.0, 0.0\n",
    "\n",
    "lr_rate = 1e-2\n",
    "n_data = len(train_x)\n",
    "errors = []\n",
    "\n",
    "for i in range(100):\n",
    "    ## Todo\n",
    "    # 예측값 y\n",
    "    _y = w*train_x[i] + b\n",
    "\n",
    "    # gradient\n",
    "    w_poly = \n",
    "    gradient_w = sym.subs(sym.diff(sym.poly((train_y[i]/train_x[i])-x*(1/train_x[i]))),b)\n",
    "    gradient_b = sym.subs(sym.diff(sym.poly(-x*train_x[i]+train_y[i])),w)\n",
    "\n",
    "    # w, b update with gradient and learning rate\n",
    "    w = w - lr_rate*gradient_w\n",
    "    b = b - lr_rate*gradient_b\n",
    "\n",
    "    # L2 norm과 np_sum 함수 활용해서 error 정의\n",
    "    error = 0\n",
    "    # Error graph 출력하기 위한 부분\n",
    "    errors.append(error)\n",
    "\n",
    "print(\"w : {} / b : {} / error : {}\".format(w, b, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqXgO77MDMvN"
   },
   "source": [
    "* 이제 plot 함수를 이용해서 full-batch gradient descent를 사용한 경우, error가 어떻게 줄어드는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPfF6tHbDQSR"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(errors):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('time step')\n",
    "    plt.plot(errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmNczAWrDRm2"
   },
   "outputs": [],
   "source": [
    "plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOU3Rq3TDVHp"
   },
   "source": [
    "### **3.2. More complicated function**\n",
    "#### **Dataset 2** : train_x, train_y\n",
    "- 이번에는 좀 더 복잡한 선형식에 대한 Regression을 진행해봅시다 !\n",
    "$$ y = w_0x_0 + w_1x_1 + w_2x_2 + b $$\n",
    "$$ y = x_0 + 3x_1 + 5x_2 + 7$$\n",
    "\n",
    "- 각 element의 계수를 beta_gd로 설정 : random initialize ( 목표 정답은 [1, 3, 5, 7] )\n",
    "- 이번에는 np.transpose 함수를 활용하여 gradient를 계산해봅시다 ( AI Math 4강 참고 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbCGTSZt1LRC"
   },
   "outputs": [],
   "source": [
    "train_x = np.array([[1,1,1], [1,1,2], [1,2,2], [2,2,3], [2,3,3], [1,2,3]])\n",
    "train_y = np.dot(train_x, np.array([1,3,5])) + 7\n",
    "\n",
    "# random initialize\n",
    "beta_gd = [9.4, 10.6, -3.7, -1.2]\n",
    "# for constant element\n",
    "expand_x = np.array([np.append(x, [1]) for x in train_x])\n",
    "\n",
    "for t in range(5000):\n",
    "    ## Todo\n",
    "\n",
    "print(\"After gradient descent, beta_gd : {}\".format(beta_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHRiic4_6LlB"
   },
   "source": [
    "## 4. Stochastic Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH4lCXrGDezY"
   },
   "source": [
    "- 3-1의 문제와 동일한 문제에 대해서 Stochastic Gradient Descent를 사용해봅시다.\n",
    "\n",
    "- Code와 dataset 모두 동일하게 사용하되, 기존의 Dataset으로부터 mini-batch를 구성해서 Gradient Descent를 진행해주시면 됩니다.\n",
    "    - mini-batch의 경우, **np.random.choice** 함수를 활용하셔서 1,000개의 dataset 중 10개를 뽑아서 만들어주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn8133mq6obv"
   },
   "outputs": [],
   "source": [
    "train_x = (np.random.rand(1000) - 0.5) * 10\n",
    "train_y = np.zeros_like(train_x)\n",
    "\n",
    "def func(val):\n",
    "    fun = sym.poly(7*x + 2)\n",
    "    return fun.subs(x, val)\n",
    "\n",
    "for i in range(1000):\n",
    "    train_y[i] = func(train_x[i])\n",
    "\n",
    "# initialize\n",
    "w, b = 0.0, 0.0\n",
    "\n",
    "lr_rate = 1e-2\n",
    "n_data = 10\n",
    "errors = []\n",
    "\n",
    "for i in range(100):\n",
    "    ## Todo\n",
    "\n",
    "\n",
    "    # Error graph 출력하기 위한 부분\n",
    "    errors.append(error)\n",
    "\n",
    "print(\"w : {} / b : {} / error : {}\".format(w, b, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZE2CsZWDj7W"
   },
   "source": [
    "- plot 함수를 이용해서 mini-batch를 사용한 stochastic gradient descent의 경우, error가 어떻게 줄어드는지 확인해봅시다.\n",
    "- 3.1의 full-batch gradient descent를 사용한 경우와 plot을 통해 비교를 해보면 차이를 좀 더 명확히 확인할 수 있습니다.\n",
    "    - full-batch의 경우, 매 epoch마다 전체 dataset을 모두 사용하여 GD를 하기 때문에 그래프가 매끄럽지만, SGD에 비하여 초기 수렴속도가 느린 편입니다.\n",
    "    - mini-batch의 경우, 매 epoch마다 mini-batch를 sampling해서 GD를 하기 때문에 그래프가 매끄럽지 않지만, 그만큼 초기에 빠르게 minimum으로 수렴하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCpqNbBsDidu"
   },
   "outputs": [],
   "source": [
    "plot(errors)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1[문제]__Gradient_Descent.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
