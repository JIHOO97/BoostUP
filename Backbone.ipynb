{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pandas import DataFrame\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vision_utils\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18002683-6aa9-426a-9a9f-51c69b700136",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeus0007\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: <ipython-input-2-5634e765c456> 4 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5634e765c456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_size_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_size_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m wandb.init(project='img-classification-38', entity='zeus0007',config = {\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"interrupted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0merror_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;31m# initiate run (stats and metadata probing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mrun_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_obj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_obj_offline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate_run_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_run_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mcommunicate_run_start\u001b[0;34m(self, run_pb)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mrun_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_pb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, rec, timeout, local)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     ) -> Optional[pb.Result]:\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_communicate_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_Future\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mis_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object_ready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_set\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model 종류 : 'resnext50_32x4d','resnext101_32x8d','vit_base_patch16_224','vgg16','resnet152','resnet34','vit_large_patch16_224','custom_model'\n",
    "img_size_x=[512,256,299]\n",
    "img_size_y=[384,192,224]\n",
    "wandb.init(project='img-classification-38', entity='zeus0007',config = {\n",
    "    'learning_rate':0.001,\n",
    "    'batch_size':16,\n",
    "    'epoch':2,\n",
    "    'model':'vit_base_patch16_224',\n",
    "    'momentum':0.9,\n",
    "    'img_x':img_size_x[2],\n",
    "    'img_y':img_size_y[2],\n",
    "    'kfold_num':3,\n",
    "})\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74ea3b-436d-4198-89d0-9b1e807a4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목 생성\n",
    "TITLE = f'm_{config.model}_sk{config.kfold_num}_e{config.epoch}_s{config.img_x}x{config.img_y}_b{config.batch_size}_l{config.learning_rate}_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1c93a-bc16-4dc8-9606-1904564d7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "TRAIN_MASK_PATH = {'label':'/opt/ml/input/data/train/train.csv','images':'/opt/ml/input/data/train/images','new':'/opt/ml/input/data/train/new_train.csv'}\n",
    "TEST_MASK_PATH = '/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd49eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22dfdb46-9964-478e-ab61-f743221f420c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d240ff2-c09f-4cd1-af4d-9b64f041d6cd",
   "metadata": {},
   "source": [
    "## Conv3x3BnRElu\n",
    "\n",
    "    -subclass from nn.module\n",
    "    -implement forward\n",
    "    -kernel size 3,3 , bn-relu block\n",
    "### member variable\n",
    "    -conv ->conv layer\n",
    "    -bn -> batcnh normalization\n",
    "### forward\n",
    "\n",
    "\n",
    "#### input : x\n",
    "        input from previous layer\n",
    "\n",
    "#### output :\n",
    "\n",
    "        output wiil be input to next layer\n",
    "        \n",
    "        \n",
    "## Conv1x1BNReLU\n",
    "\n",
    "        -subclass from nn.module\n",
    "        -implement forward\n",
    "        -kernel size 1,1 ,bn-relu block\n",
    "### member variable\n",
    "    -conv ->conv layer\n",
    "    -bn -> batcnh normalization\n",
    "    \n",
    "### forward\n",
    "\n",
    "\n",
    "#### input : x\n",
    "    input from previous layer\n",
    "\n",
    "#### output :\n",
    "\n",
    "    output wiil be input to next layer\n",
    "    \n",
    "    \n",
    "## MyModel\n",
    "\n",
    "    -subclass from torch.utils.data.Dataset\n",
    "    -implement len,getitem\n",
    "\n",
    "\n",
    "### memeber_variable\n",
    "\n",
    "    -Conv1_k ,Conv1_k (k is integer)\n",
    "        : conv 1*1 bnrelu\n",
    "    -Conv k ( k is integer)\n",
    "        : conv 3*3 bnrelu\n",
    "    - Block k : (k is integer)\n",
    "        : conv 1*1 bn-relu , conv 3*3 bn-relu\n",
    "     - avg-pool : pooling layer\n",
    "\n",
    "     -classifier : \n",
    "         : output layer\n",
    "         \n",
    "### forward\n",
    "\n",
    "\n",
    "    #### input : x\n",
    "        input image\n",
    "\n",
    "    #### output :\n",
    "\n",
    "        output ,  softmax multilabel classification  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb63e90-3998-48e0-9312-2becec9f35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom model\n",
    "class Conv3x3BNReLU(nn.Module):\n",
    "        \"\"\"\n",
    "        ## Conv3x3BnRElu\n",
    "        \n",
    "            -subclass from nn.module\n",
    "            -implement forward\n",
    "            -kernel size 3,3 , bn-relu block\n",
    "        \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, padding=1):\n",
    "        '''\n",
    "            -conv ->conv layer\n",
    "            -bn -> batcnh normalization\n",
    "        '''\n",
    "        super(Conv3x3BNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        #### input : x\n",
    "                input from previous layer\n",
    "    \n",
    "        #### output :\n",
    "\n",
    "                output wiil be input to next layer\n",
    "\n",
    "        '''\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=False)\n",
    "    \n",
    "class Conv1x1BNReLU(nn.Module):\n",
    "    \"\"\"\n",
    "        ## Conv1x1BNReLU\n",
    "\n",
    "        -subclass from nn.module\n",
    "        -implement forward\n",
    "        -kernel size 1,1 ,bn-relu block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        '''\n",
    "            -conv -> conv layer\n",
    "            -bn -> b atcnh normalization\n",
    "        '''\n",
    "        super(Conv1x1BNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            \n",
    "            ### forward\n",
    "\n",
    "\n",
    "                #### input : x\n",
    "                    input from previous layer\n",
    "\n",
    "                #### output :\n",
    "\n",
    "                    output wiil be input to next layer\n",
    "        '''\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=False)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    \"\"\"\n",
    "        ## MyModel\n",
    "\n",
    "        -subclass from torch.utils.data.Dataset\n",
    "        -implement len,getitem\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        \n",
    "        '''\n",
    "            -Conv1_k ,Conv1_k (k is integer)\n",
    "                : conv 1*1 bnrelu\n",
    "            -Conv k ( k is integer)\n",
    "                : conv 3*3 bnrelu\n",
    "            - Block k : (k is integer)\n",
    "                : conv 1*1 bn-relu , conv 3*3 bn-relu\n",
    "             - avg-pool : pooling layer\n",
    "\n",
    "             -classifier : \n",
    "                 : output layer\n",
    "         \n",
    "        '''\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.Conv1_1 = Conv3x3BNReLU(in_channels=3, out_channels=32, stride=1, padding=1)\n",
    "        self.Conv1_2 = Conv3x3BNReLU(in_channels=32, out_channels=64, stride=2)\n",
    "        self.Block1 = nn.Sequential(\n",
    "            Conv1x1BNReLU(64, 32),\n",
    "            Conv3x3BNReLU(32, 64)\n",
    "        )\n",
    "        \n",
    "        self.Conv2 = Conv3x3BNReLU(in_channels=64, out_channels=128, stride=2)\n",
    "        self.Block2 = nn.Sequential(\n",
    "            Conv1x1BNReLU(128, 64),\n",
    "            Conv3x3BNReLU(64, 128)\n",
    "        )\n",
    "        \n",
    "        self.Conv3 = Conv3x3BNReLU(in_channels=128, out_channels=256, stride=2)\n",
    "        self.Block3 = nn.Sequential(\n",
    "            Conv1x1BNReLU(256, 128),\n",
    "            Conv3x3BNReLU(128, 256)\n",
    "        )\n",
    "        \n",
    "        self.Conv4 = Conv3x3BNReLU(in_channels=256, out_channels=512, stride=2)\n",
    "        self.Block4 = nn.Sequential(\n",
    "            Conv1x1BNReLU(512, 256),\n",
    "            Conv3x3BNReLU(256, 512)\n",
    "        )\n",
    "        \n",
    "        self.Conv5 = Conv3x3BNReLU(in_channels=512, out_channels=1024, stride=2)\n",
    "        self.Block5 = nn.Sequential(\n",
    "            Conv1x1BNReLU(1024, 512),\n",
    "            Conv3x3BNReLU(512, 1024)\n",
    "        )        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        ### forward\n",
    "\n",
    "\n",
    "            #### input : x\n",
    "                input image\n",
    "\n",
    "            #### output :\n",
    "\n",
    "                output ,  softmax multilabel classification   \n",
    "\n",
    "        \n",
    "        '''\n",
    "        x = self.Conv1_1(x)\n",
    "        x = self.Conv1_2(x)\n",
    "        x_temp = x.clone()\n",
    "        x = self.Block1(x)\n",
    "        x += x_temp\n",
    "        \n",
    "        x = self.Conv2(x)\n",
    "        for i in range(2):\n",
    "            x_temp = x.clone()\n",
    "            x = self.Block2(x)\n",
    "            x += x_temp\n",
    "        \n",
    "        x = self.Conv3(x)\n",
    "        for i in range(8):\n",
    "            x_temp = x.clone()\n",
    "            x = self.Block3(x)\n",
    "            x += x_temp\n",
    "        \n",
    "        x = self.Conv4(x)\n",
    "        for i in range(8):\n",
    "            x_temp = x.clone()\n",
    "            x = self.Block4(x)\n",
    "            x += x_temp\n",
    "        \n",
    "        x = self.Conv5(x)\n",
    "        for i in range(4):\n",
    "            x_temp = x.clone()\n",
    "            x = self.Block5(x)\n",
    "            x += x_temp\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def init_param(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d): # init conv\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m,nn.BatchNorm2d): # init BN\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m,nn.Linear): # lnit dense\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea8d04-f32c-4f5d-8b8a-bdaa7dd109ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config Model\n",
    "### input\n",
    "    +input : model_name\n",
    "    +specific model name\n",
    "\n",
    "   ###     pre_classified_models : \n",
    "    +model_list\n",
    "\n",
    "\n",
    "\n",
    "   ###      output : model \n",
    "    +pretrained model from hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72575b71-2ecd-42c9-9b41-1af10e2a08f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-5a78f4c1a5f4>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5a78f4c1a5f4>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    elif model\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def config_model(model_name):\n",
    "    \"\"\"\n",
    "    ## Config Model\n",
    "\n",
    "    ###     input : model_name\n",
    "    ####       specific model name\n",
    "\n",
    "    ####     pre_classified_models : model_list\n",
    "\n",
    "\n",
    "\n",
    "    ###      output : model \n",
    "    ####            pretrained model from hub\n",
    "    \n",
    "    \"\"\"\n",
    "    pre_classified_models = ['vit_base_patch16_224','vgg16','vit_large_patch16_224','custom_model']\n",
    "    if model_name in pre_classified_models:\n",
    "        if model_name == 'vit_base_patch16_224':\n",
    "            model = timm.create_model('vit_base_patch16_224',pretrained=True,num_classes=18).to(device)\n",
    "        elif model_name == 'vgg16':\n",
    "            model = models.vgg16(pretrained=True).to(device)\n",
    "        elif model_name == 'vit_large_patch16_224':\n",
    "            model = timm.create_model('vit_large_patch16_224',pretrained=True,num_classes=18).to(device)\n",
    "        elif model\n",
    "    else :\n",
    "        if model_name == 'resnext50_32x4d':\n",
    "            model = models.resnext50_32x4d(pretrained=True).to(device)\n",
    "        elif model_name == 'resnext101_32x8d':\n",
    "            model = models.resnext101_32x8d(pretrained=True).to(device)\n",
    "        elif model_name == 'resnet152':\n",
    "            model = models.resnet152(pretrained=True).to(device)\n",
    "        elif model_name == 'resnet34':\n",
    "            model = models.resnet34(pretrained=True).to(device) \n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_features, 18).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f1e78-122d-4f6a-a688-48d1a9463750",
   "metadata": {},
   "source": [
    "## transforms\n",
    "\n",
    "### input :\n",
    "\n",
    "    -train :\n",
    "        train flag\n",
    "        \n",
    "    - img_size :\n",
    "        input W * H\n",
    "        \n",
    "     -mean :\n",
    "         R,G,B \n",
    "     -std:\n",
    "         R,G,B\n",
    "\n",
    "### transform\n",
    "    - module from albumenattions\n",
    "    \n",
    " \n",
    "\n",
    "    -Reference : https://albumentations.ai/docs/api_reference/pytorch/transforms/\n",
    "### output:\n",
    "    -transform : t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91f162-3830-46d8-8d0b-22f10644d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vit_base_patch16_224\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "def transforms(train=True, img_size=(512, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    ## transforms\n",
    "\n",
    "            ### input :\n",
    "\n",
    "                -train :\n",
    "                    train flag\n",
    "\n",
    "                - img_size :\n",
    "                    input W * H\n",
    "\n",
    "                 -mean :\n",
    "                     R,G,B \n",
    "                 -std:\n",
    "                     R,G,B\n",
    "\n",
    "            ### transform\n",
    "                - module from albumenattions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ### output:\n",
    "                -transform : t\n",
    "                \n",
    "    -Reference : https://albumentations.ai/docs/api_reference/pytorch/transforms/\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(img_size[0], img_size[1], p=1.0),\n",
    "            A.CenterCrop(224,224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(img_size[0], img_size[1]),\n",
    "            A.CenterCrop(224,224),\n",
    "            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def3df3-12ef-442c-937a-b490a8eb2ea1",
   "metadata": {},
   "source": [
    "## MaskDataset\n",
    "\n",
    "    -subclass from torch.utils.data.Dataset\n",
    "    -implement len,getitem\n",
    "### member_variable\n",
    "    -data , : \n",
    "        csv\n",
    "   - image_path:\n",
    "       image_path from csv\n",
    "   -classified labels:\n",
    "       labels\n",
    "       \n",
    "    -images_full_path :\n",
    "        image_path\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbf10b-cfe6-43d7-a460-9e71eda41c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "        ## MaskDataset\n",
    "\n",
    "        -subclass from torch.utils.data.Dataset\n",
    "        -implement len,getitem\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, df, index, train=True):\n",
    "        '''\n",
    "            -data :\n",
    "                    csv\n",
    "            \n",
    "           - image_path:\n",
    "               image_path from csv\n",
    "               \n",
    "           -classified labels:\n",
    "               labels\n",
    "\n",
    "            -images_full_path :\n",
    "                image_path\n",
    "\n",
    "        \n",
    "        '''\n",
    "        data = df.iloc[index].reset_index(drop=True)\n",
    "        image_path = data['abs_path']\n",
    "        \n",
    "        self.classified_labels = data['class']\n",
    "        self.images_full_path = image_path\n",
    "        \n",
    "        if train :\n",
    "            self.transform =transforms(img_size = (config.img_x,config.img_y),train=True)\n",
    "        else :\n",
    "            self.transform = transforms(img_size = (config.img_x,config.img_y),train=False)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images_full_path.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        image_path = self.images_full_path[idx]\n",
    "        image = Image.open('/opt/ml/'+image_path)\n",
    "        y = self.classified_labels[idx]\n",
    "        \n",
    "        X = self.transform(image=np.array(image))['image']\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee989f-b977-4c7e-9a99-e5dc143fd761",
   "metadata": {},
   "source": [
    "## MaskTestDataset\n",
    "\n",
    "    -subclass from torch.utils.data.Dataset\n",
    "    -implement len,getitem\n",
    "### member_variable\n",
    "    -data , : \n",
    "        csv\n",
    "   - image_path:\n",
    "       image_path from csv\n",
    "   -classified labels:\n",
    "       labels\n",
    "       \n",
    "    -images_full_path :\n",
    "        image_path\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745ea34-0a55-4563-9f8f-0a448d5c0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskTestDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "        ## MaskTestDataset\n",
    "\n",
    "        -subclass from torch.utils.data.Dataset\n",
    "        -implement len,getitem\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        '''\n",
    "            -data , : \n",
    "                csv\n",
    "                \n",
    "           - image_path:\n",
    "               image_path from csv\n",
    "               \n",
    "           -classified labels:\n",
    "               labels\n",
    "\n",
    "            -images_full_path :\n",
    "                image_path\n",
    "\n",
    "        \n",
    "        '''\n",
    "        data = df.reset_index(drop=True)\n",
    "        image_path = data['abs_path']\n",
    "        self.classified_labels = data['class']\n",
    "        self.images_full_path = image_path\n",
    "        self.transform = transforms(img_size = (config.img_x,config.img_y),train=False)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.images_full_path.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        image_path = self.images_full_path[idx]\n",
    "        image = Image.open('/opt/ml/'+image_path)\n",
    "        y = self.classified_labels[idx]\n",
    "        \n",
    "        X = self.transform(image=np.array(image))['image']\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e10c9-a3c8-4e9c-825e-d8e437e86e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import & split data\n",
    "df = pd.read_csv(TRAIN_MASK_PATH['new'])\n",
    "test_length = len(df) - int(len(df)*0.2)\n",
    "test_df = df.iloc[test_length:]\n",
    "train_df = df.iloc[:test_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305dae6-03bc-4950-88fd-10b2d6fa92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data settings\n",
    "test_dataset = MaskTestDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be1461-3dc0-4a33-85f5-55b4d01497dc",
   "metadata": {},
   "source": [
    "### createKfold\n",
    "\n",
    "\n",
    "\n",
    "#### input : \n",
    "        df: csv for train_data\n",
    "        (fold_type) = 'kfold' or 'stratified_kfold' -> (string)\n",
    "        (n_splits) = 0 -> (int)\n",
    "#### output :\n",
    "\n",
    "    output :\n",
    "        folded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca4e2c-7ae2-4709-a4ec-da756417380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold cross validation dataset\n",
    "def create_kfold_datasets(df, fold_type = 'stratified_kfold', n_splits = 0):\n",
    "    '''\n",
    "        (fold_type) = 'kfold' or 'stratified_kfold' -> (string)\n",
    "        (n_splits) = 0 -> (int)\n",
    "    '''\n",
    "    if fold_type == 'kfold':\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "        def fold_dataset():\n",
    "            for train_index, val_index in kfold.split(df):\n",
    "                train_dataset = MaskDataset(df, train_index, train=True)\n",
    "                val_dataset = MaskDataset(df, val_index, train=False)\n",
    "                yield train_dataset, val_dataset\n",
    "        fold_datasets = fold_dataset()\n",
    "        return fold_datasets\n",
    "    elif fold_type == 'stratified_kfold':\n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "        kfold_label = df['class']\n",
    "        def fold_dataset():\n",
    "            for train_index, val_index in kfold.split(df, kfold_label):\n",
    "                train_dataset = MaskDataset(df, train_index, train=True)\n",
    "                val_dataset = MaskDataset(df, val_index, train=False)\n",
    "                yield train_dataset, val_dataset\n",
    "        fold_datasets = fold_dataset()\n",
    "        return fold_datasets\n",
    "        \n",
    "    else : \n",
    "        print(\"Fold type error : Use 'kfold' or 'stratified_kfold' \")\n",
    "#         raise\n",
    "\n",
    "# 자동화시에 주석\n",
    "fold_datasets = create_kfold_datasets(train_df, 'stratified_kfold', config.kfold_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b94087-5de0-41ea-a655-759f54960033",
   "metadata": {},
   "source": [
    "## train_model\n",
    "\n",
    "### input:\n",
    "    model : \n",
    "        custom model\n",
    "    \n",
    "    criterion :\n",
    "        \n",
    "    optimizer:\n",
    "    \n",
    "    fold_datasets:\n",
    "    \n",
    "    num_epochs:\n",
    "    \n",
    "    \n",
    "### description:\n",
    "    train model\n",
    "    \n",
    "### output:\n",
    "    \n",
    "    traiend model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81710f19-4dee-44e9-9779-a9fb2492bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,fold_datasets, num_epochs=1,):\n",
    "    '''\n",
    "    \n",
    "    ## train_model\n",
    "\n",
    "        ### input:\n",
    "            model : \n",
    "                custom model\n",
    "\n",
    "            criterion :\n",
    "\n",
    "            optimizer:\n",
    "\n",
    "            fold_datasets:\n",
    "\n",
    "            num_epochs:\n",
    "\n",
    "\n",
    "        ### description:\n",
    "            train model\n",
    "\n",
    "        ### output:\n",
    "\n",
    "            traiend model\n",
    "\n",
    "    \n",
    "    '''\n",
    "    for i, (train_dataset, val_dataset) in enumerate(fold_datasets):\n",
    "        print(f'k-fold : {i+1}')\n",
    "        print('-' * 10)\n",
    "        image_datasets = {'train':train_dataset,'validation':val_dataset}\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=4,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=4,\n",
    "            shuffle=True\n",
    "        )\n",
    "        dataloaders = {'train':train_loader, 'validation':val_loader}\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'validation']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_loss / len(image_datasets[phase])\n",
    "                epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "                wandb.log({f\"{phase}_acc\":epoch_acc, f\"{phase}_loss\":epoch_loss})\n",
    "                print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                            epoch_loss,\n",
    "                                                            epoch_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb2a62-ebdc-4537-9de7-fd8c2e37181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 정의\n",
    "model = config_model(config.model)\n",
    "\n",
    "#Hyper parameter 가져오기\n",
    "lr = config.learning_rate\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=config.momentum)\n",
    "\n",
    "#모델 학습\n",
    "model = train_model(model, criterion, optimizer,fold_datasets, num_epochs=config.epoch)\n",
    "model.eval()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a435cba-e858-43c9-8912-8a2aae96f1bd",
   "metadata": {},
   "source": [
    "## test_model \n",
    "\n",
    "### input\n",
    "\n",
    "    model\n",
    "    \n",
    "    test_dataset\n",
    "    \n",
    "    test_loader\n",
    "    \n",
    "### description\n",
    "\n",
    "    evalution model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8f672-965c-4219-9b72-57c2e472ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,test_dataset,test_loader):\n",
    "    \"\"\"\n",
    "    \n",
    "    ## test_model \n",
    "\n",
    "        ### input\n",
    "\n",
    "            model\n",
    "\n",
    "            test_dataset\n",
    "\n",
    "            test_loader\n",
    "\n",
    "        ### description\n",
    "\n",
    "            evalution model\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    test_loss = running_loss / len(test_dataset)\n",
    "    test_acc = running_corrects.double() / len(test_dataset)\n",
    "    print('test loss: {:.4f}, acc: {:.4f}'.format(test_loss,test_acc))\n",
    "    wandb.log({f\"test_acc\":test_acc, f\"test_loss\":test_loss})\n",
    "    print('test_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39690d-fff1-4c9b-8a51-1cca20aab84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일 파일명 존재 여부 체크 & 모델 저장\n",
    "while True :\n",
    "    isfile = os.path.isfile('/opt/ml/code/model/{TITLE}.pth')\n",
    "    if isfile:\n",
    "        TITLE = TITLE[:len(TITLE)-2]+str(int(TITLE[len(TITLE)-1])+1)\n",
    "    else :\n",
    "        break\n",
    "torch.save(model.state_dict(), f'/opt/ml/code/model/{TITLE}.pth')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50014904-8e25-4508-ae5f-1d0b09a5fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트\n",
    "test_model(model,test_dataset,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d42472-86a8-41c5-b056-2dfcb3d9bd3c",
   "metadata": {},
   "source": [
    "## TestDataset\n",
    "    subclass of Dataset\n",
    "    implementation len, getitem\n",
    "### member variable\n",
    "\n",
    "    img_path\n",
    "    \n",
    "    transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15839414-70fc-45ae-b6ec-07b02df1110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ## TestDataset\n",
    "        subclass of Dataset\n",
    "        implementation len, getitem\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, img_paths, transform):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "            img_path  : Supports jpg, jpeg, png, etc.\n",
    "    \n",
    "            transform  : module albumentations\n",
    "        \n",
    "        '''\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b9581-4f82-48a8-89c3-77d681712b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset settings\n",
    "from torchvision import transforms as simple_transforms\n",
    "\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = A.Compose([\n",
    "            A.Resize(config.img_x, config.img_y),\n",
    "            A.CenterCrop(224,224),\n",
    "            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ee37d-a5d9-430b-822a-c6e8a7551c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 예측 이미지 생성\n",
    "import torchvision.utils as vision_utils\n",
    "\n",
    "mean=(0.5, 0.5, 0.5)\n",
    "std=(0.5, 0.5, 0.5)\n",
    "\n",
    "def imshow(input, title):\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# Test gender model\n",
    "model.eval()\n",
    "class_names = [0, 1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    for i, inputs in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        images = vision_utils.make_grid(inputs)\n",
    "        imshow(images.cpu(), title=preds)\n",
    "            \n",
    "        if i == 200:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d6577-7a38-4640-9d94-6f47736b2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 예측\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, f'submission_{TITLE}.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
