{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bd8f7b-5104-42cb-b5b0-e9c73d19590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from matplotlib import image\n",
    "from torchvision import transforms, datasets, models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import wandb\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ce3d3d-3e0a-44b0-ad03-6d92f2027fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeus0007\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fallen-river-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/zeus0007/img-classification\" target=\"_blank\">https://wandb.ai/zeus0007/img-classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/zeus0007/img-classification/runs/1k0aj62q\" target=\"_blank\">https://wandb.ai/zeus0007/img-classification/runs/1k0aj62q</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/wandb/run-20210903_082448-1k0aj62q</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1k0aj62q)</h1><iframe src=\"https://wandb.ai/zeus0007/img-classification/runs/1k0aj62q\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f78e3467e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='img-classification', entity='zeus0007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e945b9a4-10f8-4750-a123-838a122460c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(r'/opt/ml/input/data/train/new_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33142637-97f0-4636-a2f3-aec802802d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Path</th>\n",
       "      <th>NewClass</th>\n",
       "      <th>-</th>\n",
       "      <th>Class Compare</th>\n",
       "      <th>Original Class</th>\n",
       "      <th>-.1</th>\n",
       "      <th>Mask</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>-.2</th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Gender.1</th>\n",
       "      <th>Age.1</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>incorrect_mask.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask1.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask2.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask3.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask4.jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask2.jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>6959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask3.jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>6959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask4.jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>6959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mask5.jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>6959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal.jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>6959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Full Path  NewClass   -  \\\n",
       "0      /opt/ml/input/data/train/images/000001_female_...        10 NaN   \n",
       "1      /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "2      /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "3      /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "4      /opt/ml/input/data/train/images/000001_female_...         4 NaN   \n",
       "...                                                  ...       ...  ..   \n",
       "18895  /opt/ml/input/data/train/images/006959_male_As...         0 NaN   \n",
       "18896  /opt/ml/input/data/train/images/006959_male_As...         0 NaN   \n",
       "18897  /opt/ml/input/data/train/images/006959_male_As...         0 NaN   \n",
       "18898  /opt/ml/input/data/train/images/006959_male_As...         0 NaN   \n",
       "18899  /opt/ml/input/data/train/images/006959_male_As...        12 NaN   \n",
       "\n",
       "       Class Compare  Original Class  -.1  Mask  Gender  Age  -.2  \\\n",
       "0               True              10  NaN     1       1    1  NaN   \n",
       "1               True               4  NaN     0       1    1  NaN   \n",
       "2               True               4  NaN     0       1    1  NaN   \n",
       "3               True               4  NaN     0       1    1  NaN   \n",
       "4               True               4  NaN     0       1    1  NaN   \n",
       "...              ...             ...  ...   ...     ...  ...  ...   \n",
       "18895           True               0  NaN     0       0    0  NaN   \n",
       "18896           True               0  NaN     0       0    0  NaN   \n",
       "18897           True               0  NaN     0       0    0  NaN   \n",
       "18898           True               0  NaN     0       0    0  NaN   \n",
       "18899           True              12  NaN     2       0    0  NaN   \n",
       "\n",
       "                ImageName Gender.1  Age.1    ID  \n",
       "0      incorrect_mask.jpg   female     45     1  \n",
       "1               mask1.jpg   female     45     1  \n",
       "2               mask2.jpg   female     45     1  \n",
       "3               mask3.jpg   female     45     1  \n",
       "4               mask4.jpg   female     45     1  \n",
       "...                   ...      ...    ...   ...  \n",
       "18895           mask2.jpg     male     19  6959  \n",
       "18896           mask3.jpg     male     19  6959  \n",
       "18897           mask4.jpg     male     19  6959  \n",
       "18898           mask5.jpg     male     19  6959  \n",
       "18899          normal.jpg     male     19  6959  \n",
       "\n",
       "[18900 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bac781-1f3b-4a4b-b07a-15748973e5b3",
   "metadata": {},
   "source": [
    "## Dataset 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "572b165b-dce2-4168-a94a-0932434ac1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "class imgDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.main_path = csv_path\n",
    "        self.total_data = pd.read_csv(self.main_path + '/' + 'new_train.csv')\n",
    "        self.transform = transforms.Compose([\n",
    "                                            transforms.CenterCrop(384),\n",
    "                                            transforms.Resize(256),\n",
    "                                            transforms.ToTensor(),\n",
    "                                             \n",
    "                                            transforms.Normalize(mean = [0.5,0.5,0.5],\n",
    "                                                                 std =[0.5,0.5,0.5])])#384 384로 crop을 하고, 그런 후 256으로 resize\n",
    "    def __getitem__(self,index):\n",
    "        image = Image.open(self.total_data.iloc[index]['Full Path'])\n",
    "        image = self.transform(image)\n",
    "        class_num = self.total_data.iloc[index]['NewClass']\n",
    "        gender = self.total_data.iloc[index]['Gender'] #female 은 1 male은 0\n",
    "        mask = self.total_data.iloc[index]['Mask']# noraml이 1, mask가 0, 이상한 착용이 2\n",
    "        age = self.total_data.iloc[index]['Age']#1이30~60, 0이 30 이하, 2가 60 이상\n",
    "        return gender, mask, age, class_num, image\n",
    "    def __len__(self):\n",
    "        return self.total_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126b5bb7-2013-4ea2-bed7-2d739ac20cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_img = imgDataset(r'/opt/ml/input/data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10c06d8-0834-41ae-a3e1-10c3bfe8c5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f70835-32f3-4d58-bc15-b01a921c684e",
   "metadata": {},
   "source": [
    "## Train Valid 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f11eea64-105f-48bc-93f0-e90e2639b4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120\n",
      "3780\n"
     ]
    }
   ],
   "source": [
    "train_len = int(len(class_img) * 0.8)\n",
    "print(train_len)\n",
    "val_len = len(class_img) - train_len\n",
    "print(val_len)\n",
    "from torch.utils.data.dataset import random_split\n",
    "train_dataset, val_dataset = random_split(class_img, [train_len,val_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457837de-6687-4de1-ae92-d9c0949197eb",
   "metadata": {},
   "source": [
    "## Dataloader 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26fdd2a-63ef-49d8-8899-40e36851402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size=64, shuffle = True, drop_last=True)    #통상 첨에 시작할때 batch는 64에서 128로 잡는다  \n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size=64, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa84551-5322-4c1c-8aaf-8080d3bae35f",
   "metadata": {},
   "source": [
    "## Pretrained 모델(다중 택일) + MLP 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05fd0de7-58ce-4a9f-bdc6-ab7814b42026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bfa810d-8d1e-4a46-adf0-259d2df12155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.pretrained = models.resnet152(pretrained=True)\n",
    "        #last_layer_features = self.pretrained.fc.in_features\n",
    "        ### strip the last layer\n",
    "        self.backbone = torch.nn.Sequential(*list(self.pretrained.children())[:-1])#self.backbone은 이제 resnet에서 convolution layer만 있는 상태이다!!\n",
    "        self.gender = nn.Sequential(nn.Linear(2048,500),\n",
    "                                            nn.Linear(500,100),\n",
    "                                           nn.Linear(100,2),\n",
    "                                   nn.Softmax())\n",
    "        self.mask = nn.Sequential(nn.Linear(2048,500),\n",
    "                                            nn.Linear(500,100),\n",
    "                                           nn.Linear(100,3),\n",
    "                                 nn.Softmax())\n",
    "        self.age = nn.Sequential(nn.Linear(2048,500),\n",
    "                                            nn.Linear(500,100),\n",
    "                                           nn.Linear(100,3),\n",
    "                                nn.Softmax())\n",
    "    def forward(self, x):\n",
    "        shared = self.backbone(x).squeeze()\n",
    "        gender = self.gender(shared)\n",
    "        mask = self.mask(shared)\n",
    "        age = self.age(shared)\n",
    "        \n",
    "        return gender, mask, age\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3cc3a0a-3ce6-40df-a060-375e80901772",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9512e9-10cd-446d-8d9e-2d580409114c",
   "metadata": {},
   "source": [
    "## Train후 Valid 성능 확인후, 각 epoch마다 모델 저장(torch.save(model, os.path.join(~~~~~)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de6125b6-62e6-42a3-9282-552eae2da559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_and_Valid_check(model, model_name):\n",
    "    from tqdm.notebook import tqdm\n",
    "    MODEL_PATH =\"multi_layer_saved\"\n",
    "    \n",
    "    num_epochs = 10\n",
    "    #start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\" Training Phase \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "\n",
    "        # load a batch data of images\n",
    "        for gender, mask, age, class_num, img in tqdm(train_loader):\n",
    "            #inputs = inputs.to(device)\n",
    "            gender = gender.to(device)\n",
    "            mask = mask.to(device)\n",
    "            age = age.to(device)\n",
    "            class_num = class_num.to(device)\n",
    "            img = img.to(device)#img가 \n",
    "\n",
    "            # forward inputs and get output\n",
    "            optimizer.zero_grad()\n",
    "            gender_out, mask_out, age_out = model(img)\n",
    "            #outputs = model(img)\n",
    "            _, gender_preds = torch.max(gender_out, 1)\n",
    "            _, mask_preds = torch.max(mask_out,1)\n",
    "            _, age_preds = torch.max(age_out,1)\n",
    "            #loss = loss_func(outputs, labels)\n",
    "            gender_loss = loss_func(gender_out,gender)\n",
    "            mask_loss = loss_func(mask_out, mask)\n",
    "            age_loss = loss_func(age_out, age)\n",
    "            # get loss value and update the network weights\n",
    "            \n",
    "            #total_loss = (gender_loss + mask_loss + age_loss)/3\n",
    "            total_loss = gender_loss * 0.2 + mask_loss * 0.2 + age_loss * 0.6\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = mask_preds* 6 + gender_preds * 3 + age_preds\n",
    "            running_loss += total_loss.item() * img.size(0)\n",
    "            running_corrects += torch.sum(preds == class_num.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects / len(train_dataset) * 100.\n",
    "        print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% '.format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0\n",
    "\n",
    "            for gender, mask, age, class_num, img in tqdm(val_loader):#얼마나 진행됐고 얼마나 남았는지 알려줌\n",
    "                gender = gender.to(device)\n",
    "                mask = mask.to(device)\n",
    "                age = age.to(device)\n",
    "                class_num = class_num.to(device)\n",
    "                img = img.to(device)#img가 \n",
    "\n",
    "                #outputs = model(inputs)\n",
    "                gender_out, mask_out, age_out = model(img)\n",
    "                #_, preds = torch.max(outputs, 1)\n",
    "                _, gender_preds = torch.max(gender_out, 1)\n",
    "                _, mask_preds = torch.max(mask_out,1)\n",
    "                _, age_preds = torch.max(age_out,1)\n",
    "                #loss = loss_func(outputs, labels)\n",
    "                gender_loss = loss_func(gender_out,gender)\n",
    "                mask_loss = loss_func(mask_out, mask)\n",
    "                age_loss = loss_func(age_out, age)\n",
    "            # get loss value and update the network weights\n",
    "            \n",
    "                total_loss = (gender_loss + mask_loss + age_loss)/3\n",
    "\n",
    "                running_loss += total_loss.item() * img.size(0)\n",
    "                preds = mask_preds* 6 + gender_preds * 3 + age_preds\n",
    "                running_corrects += torch.sum(preds == class_num.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(val_dataset)\n",
    "            epoch_acc = running_corrects / len(val_dataset) * 100.\n",
    "            #wandb.log({\"val_acc\":epoch_acc, \"val_loss\":epoch_loss})\n",
    "            wandb.log({\"val_acc\":epoch_acc, \"val_loss\":epoch_loss})\n",
    "            print('[Validation #{}] Loss: {:.4f} Acc: {:.4f}% '.format(epoch, epoch_loss, epoch_acc))\n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            os.makedirs(MODEL_PATH)\n",
    "        torch.save(model, os.path.join(MODEL_PATH, f\"{model_name}_is_model_and_{10 + epoch}th_epoch.pt\"))\n",
    "        #os.path.join(MODEL_PATH, \"hand_mademodel_with_new_csv.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d48f585-2011-42de-9ebb-c7d3596e38e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10197fd929541949fd95974336541a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #0] Loss: 0.5966 Acc: 68.7963% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8313abc55e64fde921921b2a3211c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #0] Loss: 0.5637 Acc: 73.6772% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0df2746c14445dea60eeb89d1d46d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #1] Loss: 0.5879 Acc: 73.4524% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09f25634cc34543bc67531fb056e6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #1] Loss: 0.5522 Acc: 73.7566% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79c10929fea494390512ec73d614957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #2] Loss: 0.5826 Acc: 73.4788% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211d06857a6a4fd68e5b6ce43680598a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #2] Loss: 0.5480 Acc: 73.5714% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9af39d738240b8b6702aaa82d1db91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #3] Loss: 0.5785 Acc: 73.7037% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbf89f996694cc784e3fdc46924901e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #3] Loss: 0.5411 Acc: 74.0741% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fb350a4b94480999258e8759b8e4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #4] Loss: 0.5759 Acc: 73.7765% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a71c3616d548c48fbf095257dfe2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #4] Loss: 0.5381 Acc: 73.8889% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3b9ac9eab149b1848684cfe242e3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #5] Loss: 0.5733 Acc: 73.8823% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92f03fac1ef4236b7c0ac1b9798f103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #5] Loss: 0.5342 Acc: 74.1270% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bd7125105148c9906a0222dfbd16fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #6] Loss: 0.5705 Acc: 76.6005% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1ac56b27a54d2696a60539b8301ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #6] Loss: 0.5302 Acc: 79.8413% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4399657956d84983818059d03a9d9956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #7] Loss: 0.5678 Acc: 81.8320% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5282f801722244f6a22dee410892a5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #7] Loss: 0.5254 Acc: 84.7090% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6916bc1532b14e4b8dec692f3ffdf244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #8] Loss: 0.5644 Acc: 84.5238% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785016898d3741e6b390fb05b4c8c976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #8] Loss: 0.5198 Acc: 85.2381% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a26dd361ced496c8c80d3de766e2518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train #9] Loss: 0.5613 Acc: 85.0727% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351a0b8b53ec47809f3181d106058cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation #9] Loss: 0.5146 Acc: 85.3439% \n"
     ]
    }
   ],
   "source": [
    "Train_and_Valid_check(model, \"MultiLayer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23513184-cb23-4226-91f1-c7e2f812a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b998371-d575-4eb6-903a-19c6df7725d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'\n",
    "from torch.utils.data import DataLoader\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                    transforms.CenterCrop(384),\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.ToTensor(),\n",
    "                                             \n",
    "                    transforms.Normalize(mean = [0.5,0.5,0.5],std =[0.5,0.5,0.5])\n",
    "                                ])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "#model = MyModel(num_classes=18).to(device)\n",
    "#model = torch.load(os.path.join(MODEL_PATH, \"model_pickle.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
